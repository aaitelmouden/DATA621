---
title: "Multiple linear regression and binary logistic regression models"
author: "Abdellah AitElmouden | Gabriel Abreu |  Jered Ataky | Patrick Maloney"
date: "4/13/2021"
output:
  pdf_document: 
    latex_engine: xelatex
    toc: true
    toc_depth: 2
    
---
```{r include=FALSE}

knitr::opts_chunk$set(echo = TRUE)
```

```{r include=FALSE}

library(corrplot)
library(tidyverse)
library(Hmisc)
library(PerformanceAnalytics)
library(mice)
library(gt)
library(DMwR2)
library(caret)
library(bnstruct)
library(VIM)
library(corrr)
library(gtsummary)
library(kableExtra)
library(naniar)
library(rpart)
library(skimr)
library(pROC)
library(MASS)
library(dplyr)
library(jtools)
library(ggcorrplot)
library(glmulti)
library(tibble)
library(performance)

```


## Introduction

The aim of this assignment is to build a binary logistic regression model to predict whether a neighborhood will be at risk for high crime levels, using a data set containing information on crime for various neighborhoods of a major city. Each record has a response variable indicating whether or not the crime rate is above the median crime rate (1) or not (0). before building the model we will perform some Exploratory Data Analysis (EDA): To visualize distributions and draw correlations between attributes. There are few issues in the data. Although there are no missing values but most of the variables seem to be skewed and not normally distributed, hence we use log transformation to make them symmetric. The obtained model were tested on criminal evaluation data. We compared the results of model predictions and selected the best binary logistic regression model.


## DATA EXPLORATION

In this section, we are going to explore the data to see the data type and data structure, We will also check the correlation among the variables and most importantly to see if there are missing values in the data. 

Both training and evaluation datasets have been read using read.csv function and above table is a sample of training dataset. we can see that the data is composed of 466 observations and 12 predictor variables. The response variable target is binary (0 or 1). All observations in this dataset are complete.


```{r}

# load the data 

raw_train_data <- read.csv("https://raw.githubusercontent.com/aaitelmouden/DATA621/master/Homework3/data/crime-training-data_modified.csv")

raw_test_data <- read.csv("https://raw.githubusercontent.com/aaitelmouden/DATA621/master/Homework3/data/crime-evaluation-data_modified.csv")

cp_data <- raw_train_data


glimpse(raw_train_data)
```
Now let's explore the data structure using skim function from skimr package. This is an efficient function which not only produces the statistics summary but also builds histogram for each numberic variable, show number of missing values and quantiles. This gives a bird eye view of the training dataset.

```{r}

# getting useful summary statistics
#skim(raw_train_data)

table1 <- tbl_summary(raw_train_data,
          statistic = list(all_continuous() ~ "{mean} ({sd}) {median} {min} {max}"), missing = "no") 
table1
```

\newpage

Proportion of target among the data. 0 means the crime rate below the median crime rate while 1 is above the median crime rate.

```{r}
raw_train_data$target <- factor(raw_train_data$target)

raw_train_data %>% 
  ggplot(aes(x=target, y = ..prop.., group = 1)) + 
  geom_bar(fill = 'firebrick', color = 'black')  +
  geom_text(stat='count', aes(label=..count..), vjust=-1) + 
  xlab("Median Crime Rate") +
  ylab("Proportion") +
  ylim(0, 0.55)

```
Looking at the histograms and box plots of each we can observe that:

- **age**: The box plot shows that generally older homes in neighborhoods are see as associated with higher crime.
- **chas**: most homes in the dataset border the Charles River, thus this may not be a good predictor variable 
- **dis**: a right skewed distribution where a lower distance to employment centers shows a higher crime indicator
- **indus**: bi-modal distribution of industrial sectors and generally seen by the box plots that the higher industrial activity results in an increased crime factor
- **lstat**: a predictor variable based on “status” of population. However it is ambigious what the sale in this factor reflects, but the observation is that the higher on the lstat scale the more indicator of crime
- **mdev**: median value of homes, and seems correct that we would see higher value homes associated with lower crimes
- **nox**: the amount of nitrogen oxides concentrations is right skewed with most locations not having a “high” amount, and as the concentration increases as does the crime
- **ptratio**: student to teacher ratio, as convention and observation show a high student to teacher ratio is indicative of higher crimes
- **rad**: the distance to highways seems slightly bi-modal, and higher distance from highways seems to be associated with higher crime, however the variability on the positive crime indicator is very large
- **rm**: the average number of rooms per home looks normally distributed and the association with crime seems evenly distributed as per the box plot
- **tax**: the property tax variable is bi-modal, the box plot shows that the variability of a positive crime indicator is fairly large
- **zn**: large lot zones show most values as 0 and lower proportions seems associated with higher crime


```{r}
raw_train_data %>%
  dplyr::select(-chas) %>% 
  gather(key, value, -target) %>% 
  mutate(key = factor(key)) %>% 
  ggplot(aes(x = key, y = value)) +
  geom_boxplot(aes(fill = target)) +
  facet_wrap(~ key, scales = 'free', ncol = 3)
```


```{r}
raw_train_data %>%
  gather(key, value, -c(target)) %>%
  ggplot(aes(value)) +
  geom_histogram() +
  facet_wrap(~ key, scales = 'free', ncol = 3)

```
\newpage

### Correlations among predictors and Variable Selection


```{r echo=FALSE, message=FALSE, warning=FALSE}

#pairwise.complete.obs ignores NA values and computes correlation on complete observations
#we might have to run these corrplots again after we handle the NA values
chart.Correlation(cp_data, histograme=TRUE, method= "pearson", use="pairwise.complete.obs")
```
\newpage

**Correlation using ggcorrplot**

```{r echo=FALSE, message=FALSE, warning=FALSE}

q <- cor(cp_data)
ggcorrplot(q, type = "upper", outline.color = "white",
           ggtheme = theme_classic,
           colors = c("#6D9EC1", "white", "#E46726"),
           lab = TRUE, show.legend = FALSE, tl.cex = 8, lab_size = 3)
```
According to correlation plot the target variable is positively correlated with nox(.73), age(.63), rad(.63), and tax(.61). Also we can see the target variable is negatively correlated with dis(-.62) as seen in the histogram and box plots, the chas variable as a very weak correlation with all the other variables, and including the target. Therefore we can look to eliminate it from the analysis. We also noticed that there is present a amount of correlation amongst the predictor variables and this is suspect for multicollinearity issues



## Data Preparation

```{r echo=FALSE, message=FALSE, warning=FALSE}
#Eliminating the chas variable

train_data <- raw_train_data %>% dplyr::select(-chas)
train_data$target <- factor(train_data$target)
```

```{r}
set.seed(123)

training.samples <- train_data$target %>% createDataPartition(p= 0.8, list = FALSE)

train.data <- train_data[training.samples, ]
test.data <- train_data[-training.samples, ]
```

<!-- Performing a box cox transformation -->
<!-- ```{r} -->
<!-- train_boxcox <- preProcess(train_data, c("BoxCox")) -->
<!-- td_bx <- predict(train_boxcox, train_data) -->
<!-- ``` -->

\newpage

## BUILD MODELS


### Model 1 : Base Model Subsets

The first model is the simplest. Run the logistic regression formula with all the variables as predictors and manually eliminate variables based on significance.


```{r}

model1 <- glm(target~., family=binomial, data=raw_train_data)
summary(model1)

glm.probs <- predict(model1, type="response")

# Confirm the 0.5 threshold
glm.pred <- ifelse(glm.probs > 0.5, 1, 0)
results <- tibble(target=raw_train_data$target, pred=glm.pred)
results <- results %>%
mutate(pred.class = as.factor(pred), target.class = as.factor(target))

```

As we can see, summary() returns the estimate, standard errors, z-score, and p-values on each of the coefficients. Look like some of the coefficients are non-significant here  (p > 0.05). chas, lstat, rm, indus, ptratio, tax and dis. And it seems like only age, nox, rad and medv have significant impact on target.


The summary shows also the null deviance (the deviance just for the mean) and the residual deviance (the deviance for the model with all the predictors). There's a large difference between the 2, along with 12 degrees of freedom.

\newpage
### Prediction

We will plot the ROC curve for the predictive result. ROC in logistic regression are used for determining the best cutoff value for predicting whether a new observation is a "failure" (0) or a "success" (1). the ROC curve shows graphically the tradeoff that occurs between trying to maximize the true positive rate vs. trying to minimize the false positive rate. In an ideal situation, you would have sensitivity and specificity near 100% at all cutoffs, meaning you predict perfectly in all cases. which is not the case for our data.

```{r echo=FALSE, message=FALSE, warning=FALSE}
p <- plot(roc(results$target.class,glm.probs), print.auc = TRUE)
```

Using the summ() function we can display model1 regression summary. the first model shows an R square of 0.83. The AIC and the BIC are 218.05, 271.92 alternatively. 

```{r}
summ(model1)
```


### Model 2 : Using Stepwise Regression

Although we have used backward elimination in which we eliminated insignificant variables one by one from the model as discussed before. We can use step() function which is more robust and it is used for stepwise regression. Basically, it eliminates all the insignificant variables one-by-one under the hood and brings the significant variables. This model is used only to verify the result of Model1 using step-wise regression

```{r, message=FALSE,warning=FALSE}

model2 <- step(model1)
summ(model2)
```
From the step() output the last Step table is the model proposed, the output also shows the "Call" function **target ~ zn + nox + age + dis + rad + tax + ptratio + medv**, which describes the actual model and what input variables it includes, and the "Coefficients" are the actual parameter estimates for these values.

From the summ() output above, it can be seen that:

- The two models have almost the same Pseudo, meaning that they are equivalent in explaining the outcome. However, the model 2 is more simple than model 1 because it incorporates less variables. All things equal, the simple model is always better in statistics.

- The AIC and the BIC of the model 2 are lower than those of the model1. In model comparison strategies, the model with the lowest AIC and BIC score is preferred.

- Finally, the F-statistic p.value of the model 2 is lower than the one of the model 1. This means that the model 2 is statistically more significant compared to model 1, which is consistent to the above conclusion.

\newpage

## Model 3 Using glmulti: 

glmulti finds what are the n best models (the confidence set of models) among all possible models. Models are fitted with the specified fitting function (default is glm) and are ranked with the specified Information Criterion (default is aicc). The output can be used for model selection, variable selection, and multimodel inference. It takes few time to optimize the model though. We will see which model performed best in terms of performance and accuracy in the next section. 

```{r, message=FALSE, warning=FALSE, include=FALSE}
# Model4 using glmulti()

model3 <- glmulti(target ~ ., data = raw_train_data, level = 1, method="h", crit = "aic", plotty = FALSE, fitfunction = "glm", family=binomial)
```

```{r, message=FALSE, warning=FALSE}
print(model3)
```

```{r}
plot(model3)
```
The horizontal red line differentiates between models whose AICc value is less versus more than 2 units away from that of the "best" model (i.e., the model with the lowest AICc). The output above shows that there are 14 such models. Sometimes this is taken as a cutoff, so that models with values more than 2 units away are considered substantially less plausible than those with AICc values closer to that of the best model.

```{r}
top <- weightable(model3)
top <- top[top$aic <= min(top$aic) + 2,]
top
```

We see that the "best" model is the one that only exculde lstat as a moderator. The second best includes lstat. And so on with other variables. The values under weights are the model weights (also called "Akaike weights"). From an information-theoretic perspective, the Akaike weight for a particular model can be regarded as the probability that the model is the best model.

**Variable Importance**

it may be better to ask about the relative importance of the various predictors more generally, taking all of the models into consideration. We can obtain a plot of the relative importance of the various model terms with: 

```{r}
plot(model3, type="s")
```

The importance value for a particular predictor is equal to the sum of the weights/probabilities for the models in which the variable appears. So, a variable that shows up in lots of models with large weights will receive a high importance value. The vertical red line is drawn at 0.8, which is sometimes used as a cutoff to differentiate between important and not so important variables

## SELECT MODELS

## Model Performance

In this section, we are going to select the best model out of all through using **compare_performance** and **model_performance** functions from performance package. The functions calculates AIC, BIC, R2 & adjusted r-sq, RMSE, BF and Performance_Score. If we take a look at first two models, model2 is doing great as we saw before the values of AIC and BIC both are lower. RMSE are almost the same. Model3 was calculated through glmulti() package which optimizes the model and gets the best. The value of AIC and BIC, and RMSE are almost the same as model2. We can say that Model2 is the best performing model in terms of AIC, BIC and R2 and hence we will select Model2.


```{r, message=FALSE,warning=FALSE}

compare_performance(model1, model2, rank = TRUE) %>% kable() %>% kable_styling()
model_performance(model3@objects[[1]]) %>% kable() %>% kable_styling()

```

Based on model mt1, below is the prediction distribution of the test dataset, we see that the distribution is fairly split between the binary variable target

```{r}

test_predict <- predict(model1, newdata=raw_test_data)
test_predict <- ifelse(test_predict<.5,0,1)
raw_test_data$target <- test_predict
ggplot(raw_test_data, aes(x=index(raw_test_data), y=target, color=factor(target))) + geom_point() +
  labs(x="Observation", y="target", title = "Model2 Prediction", colour = "target")
```
```{r}
table(test_predict)
```
```{r}
write.csv(test_predict, "CrimePredictions.csv")
```



## References

- [Model Selection using the glmulti and MuMIn Packages](https://www.metafor-project.org/doku.php/tips:model_selection_with_glmulti_and_mumin)
- [Regression Model Validation ](http://www.sthda.com/english/articles/38-regression-model-validation/158-regression-model-accuracy-metrics-r-square-aic-bic-cp-and-more/)
- [Binary Logistic Regression](https://towardsdatascience.com/implementing-binary-logistic-regression-in-r-7d802a9d98fe)
- [What are pseudo R-squareds?] (https://stats.idre.ucla.edu/other/mult-pkg/faq/general/faq-what-are-pseudo-r-squareds/)


