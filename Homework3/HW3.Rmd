---
title: "Multiple linear regression and binary logistic regression models"
author: "Abdellah AitElmouden | Gabriel Abreu |  Jered Ataky | Patrick Maloney"
date: "4/13/2021"
output:
  pdf_document: 
    latex_engine: xelatex
    
  
---
```{r include=FALSE}

knitr::opts_chunk$set(echo = TRUE)
```

```{r include=FALSE}

library(corrplot)
library(tidyverse)
library(Hmisc)
library(PerformanceAnalytics)
library(mice)
library(gt)
library(DMwR2)
library(caret)
library(bnstruct)
library(VIM)
library(corrr)
library(gtsummary)
library(kableExtra)
library(naniar)
library(rpart)
library(skimr)

```


## Introduction

The aim of this assignment is to build a binary logistic regression model to predict whether a neighborhood will be at risk for high crime levels, using a data set containing information on crime for various neighborhoods of a major city. Each record has a response variable indicating whether or not the crime rate is above the median crime rate (1) or not (0). before building the model we will perform some Exploratory Data Analysis (EDA): To visualize distributions and draw correlations between attributes. There are few issues in the data. Although there are no missing values but most of the variables seem to be skewed and not normally distributed, hence we use log transformation to make them symmetric. The obtained model were tested on criminal evaluation data. We compared the results of model predictions and selected the best binary logistic regression model.


## DATA EXPLORATION

In this section, we are going to explore the data to see the data type and data structure, We will also check the correlation among the variables and most importantly to see if there are missing values in the data. 

Both training and evaluation datasets have been read using read.csv function and above table is a sample of training dataset. we can see that the data is composed of 466 observations and 12 predictor variables. The response variable target is binary (0 or 1). All observations in this dataset are complete.


```{r include=true, echo=FALSE, message=FALSE, warning=FALSE}

# load the data 

raw_train_data <- read.csv("./data/crime-training-data_modified.csv")
raw_test_data <- read.csv("./data/crime-evaluation-data_modified.csv")

cp_data <- raw_train_data

glimpse(raw_train_data)
```
Now let's explore the data structure using skim function from skimr package. This is an efficient function which not only produces the statistics summary but also builds histogram for each numberic variable, show number of missing values and quantiles. This gives a bird eye view of the training dataset.

```{r}
# getting useful summary statistics
skim(raw_train_data)
```

\newpage

Proportion of target among the data. 0 means the crime rate below the median crime rate while 1 is above the median crime rate.

```{r}
raw_train_data$target <- factor(raw_train_data$target)

raw_train_data %>% 
  ggplot(aes(x=target, y = ..prop.., group = 1)) + 
  geom_bar(fill = 'firebrick', color = 'black')  +
  geom_text(stat='count', aes(label=..count..), vjust=-1) + 
  xlab("Median Crime Rate") +
  ylab("Proportion") +
  ylim(0, 0.55)

```
Looking at the histograms and box plots of each we can observe that:

- **age**: The box plot shows that generally older homes in neighborhoods are see as associated with higher crime.
- **chas**: most homes in the dataset border the Charles River, thus this may not be a good predictor variable 
- **dis**: a right skewed distribution where a lower distance to employment centers shows a higher crime indicator
- **indus**: bi-modal distribution of industrial sectors and generally seen by the box plots that the higher industrial activity results in an increased crime factor
- **lstat**: a predictor variable based on “status” of population. However it is ambigious what the sale in this factor reflects, but the observation is that the higher on the lstat scale the more indicator of crime
- **mdev**: median value of homes, and seems correct that we would see higher value homes associated with lower crimes
- **nox**: the amount of nitrogen oxides concentrations is right skewed with most locations not having a “high” amount, and as the concentration increases as does the crime
- **ptratio**: student to teacher ratio, as convention and observation show a high student to teacher ratio is indicative of higher crimes
- **rad**: the distance to highways seems slightly bi-modal, and higher distance from highways seems to be associated with higher crime, however the variability on the positive crime indicator is very large
- **rm**: the average number of rooms per home looks normally distributed and the association with crime seems evenly distributed as per the box plot
- **tax**: the property tax variable is bi-modal, the box plot shows that the variability of a positive crime indicator is fairly large
- **zn**: large lot zones show most values as 0 and lower proportions seems associated with higher crime


```{r}
raw_train_data %>%
  dplyr::select(-chas) %>% 
  gather(key, value, -target) %>% 
  mutate(key = factor(key)) %>% 
  ggplot(aes(x = key, y = value)) +
  geom_boxplot(aes(fill = target)) +
  facet_wrap(~ key, scales = 'free', ncol = 3)
```
```{r}
raw_train_data %>%
  gather(key, value, -c(target)) %>%
  ggplot(aes(value)) +
  geom_histogram() +
  facet_wrap(~ key, scales = 'free', ncol = 3)

```

### Correlations among predictors and Variable Selection



Note i wasn't able to run the code i got the error saying : Object is not a `skim_df`: missing column `skim_type`

I add a new code for the correlations using ggcorrplot function here is a documentation
http://www.sthda.com/english/wiki/ggcorrplot-visualization-of-a-correlation-matrix-using-ggplot2
--------------------------------------------------------------------------------------


```{r echo=FALSE, message=FALSE, warning=FALSE}
COR <- cp_data %>% 
  correlate() %>% 
  focus(target)
gt(COR)
```

```{r echo=FALSE, message=FALSE, warning=FALSE}
COR %>% 
  mutate(rowname = factor(term, levels = term[order(target)])) %>%  # Order by correlation strength
  ggplot(aes(x = rowname, y = target)) +
    geom_bar(stat = "identity") +
    ylab("Correlation with TARGET WINS") +
    xlab("Variables") + theme(axis.text.x = element_text(angle = 90, vjust = 0.5, hjust=1))+ ggtitle("Figure 4: Correlation Against Target Win")
```


```{r echo=FALSE, message=FALSE, warning=FALSE}

#pairwise.complete.obs ignores NA values and computes correlation on complete observations
#we might have to run these corrplots again after we handle the NA values
chart.Correlation(cp_data, histograme=TRUE, method= "pearson", use="pairwise.complete.obs")
```

--------------------------------------------------------------------------------------

**Correlation using ggcorrplot**

```{r echo=FALSE, message=FALSE, warning=FALSE}
library(ggcorrplot)

q <- cor(cp_data)
ggcorrplot(q, type = "upper", outline.color = "white",
           ggtheme = theme_classic,
           colors = c("#6D9EC1", "white", "#E46726"),
           lab = TRUE, show.legend = FALSE, tl.cex = 8, lab_size = 3)
```
According to correlation plot the target variable is positively correlated with nox(.73), age(.63), rad(.63), and tax(.61). Also we can see the target variable is negatively correlated with dis(-.62) as seen in the histogram and box plots, the chas variable as a very weak correlation with all the other variables, and including the target. Therefore we can look to eliminate it from the analysis. We also noticed that there is present a amount of correlation amongst the predictor variables and this is suspect for multicollinearity issues



## Data Preparation

```{r echo=FALSE, message=FALSE, warning=FALSE}
#Eliminating the chas variable
train_data <- raw_train_data %>% dplyr::select(-chas)
train_data$target <- factor(train_data$target)
```

```{r}
set.seed(123)

training.samples <- train_data$target %>% createDataPartition(p= 0.8, list = FALSE)

train.data <- train_data[training.samples, ]
test.data <- train_data[-training.samples, ]
```

<!-- Performing a box cox transformation -->
<!-- ```{r} -->
<!-- train_boxcox <- preProcess(train_data, c("BoxCox")) -->
<!-- td_bx <- predict(train_boxcox, train_data) -->
<!-- ``` -->


## BUILD MODELS


### Model 1
The first model is the simplest. Run the logistic regression formula with all the variables as predictors and manually eliminate variables based on significance.

```{r}
model1 <- glm(target~., family=binomial, data=train.data)
summary(model1)
```

```{r}
summary(model1$fitted.values)
```

```{r}
probabilities <- model1 %>% predict(test.data, type = "response")
predicted1.classes <- ifelse(probabilities > 0.5, "1", "0")
predicted1.classes
```

```{r}
contrasts(test.data$target)
```

```{r}
mean(predicted1.classes == test.data$target)
```

### Model 2
```{r}
model2 <- stepAIC(model1)
summary(model2)
```

```{r}
summary(model2$fitted.values)
```

```{r}
probabilities <- model2 %>% predict(test.data, type = "response")
predicted.classes <- ifelse(probabilities > 0.5, "1", "0")
predicted.classes
```

```{r}
contrasts(test.data$target)
```

```{r}
mean(predicted.classes == test.data$target)
```


## SELECT MODELS



\begin{center}
Mean (SD) Median Minimum Maximum 
\end{center}


