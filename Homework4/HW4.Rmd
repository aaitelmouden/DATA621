---
title: "Multiple linear regression and binary logistic regression models"
author: "Abdellah AitElmouden | Gabriel Abreu |  Jered Ataky | Patrick Maloney"
date: "4/13/2021"
output:
  html_document:
    df_print: paged
  pdf_document:
    latex_engine: xelatex
---

```{r include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

```{r include=FALSE}

library(corrplot)
library(tidyverse)
library(Hmisc)
library(PerformanceAnalytics)
library(mice)
library(gt)
library(caret)
library(bnstruct)
library(VIM)
library(corrr)
library(kableExtra)
library(rpart)
library(gtsummary)
library(reshape)
library(pROC)
library(randomForest)

```


## Overview

This assignment is about to explore, to analyze and to model a data set containing approximately 8000
records representing a customer at an auto insurance company. Each record has two response variables. The
first response variable, TARGET_FLAG, is a 1 or a 0. A “1” means that the person was in a car crash. A zero
means that the person was not in a car crash. The second response variable is TARGET_AMT. This value is zero
if the person did not crash their car. But if they did crash their car, this number will be a value greater than zero.
Your objective is to build multiple linear regression and binary logistic regression models on the df1ing data
to predict the probability that a person will crash their car and also the amount of money it will cost if the person
does crash their car. You can only use the variables given to you (or variables that you derive from the variables
provided).

## Data exploration


Below is a short description of the variables of interest in the data set:





```{r include=FALSE}

#Import data 
df1_data <- read.csv("https://raw.githubusercontent.com/aaitelmouden/DATA621/master/Homework4/data/insurance_training_data.csv")


```


```{r}
df <- df1_data
glimpse(df)

```


```{r include=FALSE}

# Get rid of INDEX, TARGET_FLAG, TARGET_AMT
df1 <- subset(df, select = -c(INDEX))

# Select money variables to clean before summary

df2 <- df1[, names(df1) %in% c("INCOME", "OLDCLAIM", "HOME_VAL", "BLUEBOOK")]

# Clean using string function

df2 <- apply(df2, 2, function(y) gsub("\\$","",y))
df2 <- apply(df2, 2, function(y) gsub(",","",y))
df2 <- apply(df2, 2, as.integer)

# Combine variables

df1 <- df1[, !names(df1) %in% c("INCOME", "OLDCLAIM", "HOME_VAL", "BLUEBOOK")]
df1 <- cbind(df1, df2)

# Select characters variables to clean before summary

df2 <- df1[, names(df1) %in% c("MSTATUS", "SEX", "EDUCATION", "JOB", "CAR_TYPE", "URBANICITY")]

# Clean using character function

df2 <- apply(df2, 2, function(y) gsub("z_","",y))

df1 <- df1[,!names(df1) %in% c("MSTATUS", "SEX", "EDUCATION", "JOB", "CAR_TYPE", "URBANICITY")]
df1 <- cbind(df1, df2)

```


```{r}

head(df1)
```


\newpage

 

\begin{center}
Figure 1 :  Summary Statistics
\end{center}



```{r echo=FALSE, message=FALSE, warning=FALSE}

df1$SEX <- factor(df1$SEX)
df1$PARENT1 <- factor(df1$PARENT1)
df1$CAR_USE <- factor(df1$CAR_USE)
df1$RED_CAR <- factor(df1$RED_CAR)
df1$REVOKED <- factor(df1$REVOKED)
df1$MSTATUS <- factor(df1$MSTATUS)
df1$EDUCATION <- factor(df1$EDUCATION)
df1$CAR_TYPE <- factor(df1$CAR_TYPE)
df1$MSTATUS <- factor(df1$MSTATUS)
df1$URBANICITY <- factor(df1$URBANICITY)
df1$JOB <- factor(df1$JOB)


```




```{r}

# Summary table

table1 <- tbl_summary(df1,
          statistic = list(all_continuous() ~ "{mean} ({sd}) {median} {min} {max}"), missing = "no") 
table1
```


Visualize...


```{r}

# Histogram
ggplot(melt(df1), aes(x=value)) + facet_wrap(~variable, scale="free") + geom_histogram(bins=50)

```



```{r}

# Boxplot

ggplot(melt(df1), aes(x=factor(variable), y=value)) + facet_wrap(~variable, scale="free") + geom_boxplot()


```



\begin{center}
Mean (SD) Median Minimum Maximum 
\end{center}



### Outliers

The following diagram shows the outliers for all the variables(numerical), both dependent and independent.

```{r echo=FALSE, message=FALSE, warning=FALSE}

ggplot(stack(df1), aes(x = ind, y = values, fill=ind)) + 
  geom_boxplot(outlier.colour = "red",  outlier.alpha=.4) +
  coord_cartesian(ylim = c(0, 1000)) +
  theme_classic()+
  theme(axis.text.x=element_text(angle=45, hjust=1)) 
```






### Correlation

```{r echo=FALSE, message=FALSE, warning=FALSE}

# Correlation between the numerical variables

df3 <- df1[,!names(df1) %in% c("PARENT1","RED_CAR", "REVOKED","URBANICITY","INCOME","OLDCLAIM","HOME_VAL","BLUEBOOK","SEX","MSTATUS","EDUCATION","JOB","CAR_USE","CAR_TYPE")]

a <- cor(df3, method="pearson", use="complete.obs")
a


```




```{r echo=FALSE, message=FALSE, warning=FALSE}

corrplot(a)

```


There is a little bit of correlation between HomeKids and Kidsdriv, 
and MVR_PTS and CLM_FREQ



## Data Preparation

### Missing values

```{r echo=FALSE, message=FALSE, warning=FALSE}

# Number of missing values in each variables

rev(sort(colSums(sapply(df1, is.na))))

```

```{r echo=FALSE, message=FALSE, warning=FALSE}

# Total number of missing values

sum(is.na(df1))

```

We will use mean and BoxCox transformation for data imputation,
but we need to transform little the data for BoxCox transformation...
BoxCox needs variable to have value greater or equal to 1


Prepare for BoxCox imputation...

```{r echo=FALSE, message=FALSE, warning=FALSE}

# Imputing other NA variables by median before checking on minimum

df1[is.na(df1$HOME_VAL),]$HOME_VAL <- median(df1[complete.cases(df1$HOME_VAL),]$HOME_VAL)
df1[is.na(df1$YOJ),]$YOJ <- median(df1[complete.cases(df1$YOJ),]$YOJ)
df1[is.na(df1$INCOME),]$INCOME <- median(df1[complete.cases(df1$INCOME),]$INCOME)
df1[is.na(df1$AGE),]$AGE <- median(df1[complete.cases(df1$AGE),]$AGE)

```



```{r echo=FALSE, message=FALSE, warning=FALSE}

# Looking at the minimum values in order to adjust for the BoxCox Transformation

apply(df1,2,min)


```

```{r echo=FALSE, message=FALSE, warning=FALSE}

# BoxCox Transformation

df4 <- df1 %>% 
  filter(CAR_AGE >= 0) %>% 
  mutate(CAR_AGE = CAR_AGE + 1, YOJ = YOJ + 1, INCOME = INCOME + 1) %>% 
  select(TIF, BLUEBOOK, TRAVTIME, AGE, CAR_AGE, YOJ, INCOME)

apply(df4, 2, BoxCoxTrans)


```


## Build Models


### Multiple Linear Regression Models


#### Model 1: Mean full model

```{r}

df_lm <- df1 %>% select(-TARGET_FLAG) %>% filter(TARGET_AMT > 0)

# Linear regression

lm_model1 <- lm(TARGET_AMT~.,data = df_lm)

summary(lm_model1)

```

```{r}
opar <- par(mfrow = c(2,2), oma = c(0, 0, 1.1, 0))
plot(lm_model1, las = 1)

```


#### Model 2: Stepwise

```{r}
lm_model2 <- step(lm_model1)
summary(lm_model2)

```
### Logistic Regression Models



#### Model 1: Mean full model

```{r}

df_rm <- df1 %>% select(-TARGET_AMT)

# Logistic Model

rm_model1 <- glm(TARGET_FLAG ~ ., family="binomial", df_rm)

summary(rm_model1)


```

```{r}
hist(rm_model1$fitted.values,main = " Histogram ",xlab = "Probability of Crashing", col = 'light blue')
```
```{r}
probabilities <- predict(rm_model1, df_rm, type = "response")
predicted1.classes <- ifelse(probabilities > 0.5, 1, 0)
df_rm$pred1.class <- predicted1.classes
table("Predictions" = df_rm$pred1.class, "Actual" = df_rm$TARGET_FLAG)
```
```{r}
confusionMatrix(as.factor(predicted1.classes), as.factor(df_rm$TARGET_FLAG))

```


```{r}
curve <- roc(response = df_rm$TARGET_FLAG, 
    predictor = predicted1.classes, 
    plot = TRUE, 
    print.auc = TRUE, 
    main = "ROC Curve")

```


#### Model 2: Random forest
```{r}
df_rf <- subset(df_rm, select= -c(pred1.class))

trainIndex <- createDataPartition(df_rf$TARGET_FLAG, p = .8, 
                                  list = FALSE, 
                                  times = 1)
rf_train <- df_rf[ trainIndex,]
rf_test  <- df_rf[-trainIndex,]

rf <- randomForest(factor(TARGET_FLAG) ~ ., data = rf_train, na.action = na.omit)
rf
varImpPlot(rf)
```
```{r}
test_rf <- predict(rf, rf_test)
confusionMatrix(test_rf, factor(rf_test$TARGET_FLAG), positive = '1')
```



#### Model 3: Stepwise
```{r}
no_na_df <- na.omit(rf_train)
rm_model1 <- glm(TARGET_FLAG ~ ., family="binomial", no_na_df)

```

```{r}
model2 <- step(rm_model1)

```

```{r}
summary(model2)
```

```{r}
hist(model2$fitted.values,main = " Histogram ",xlab = "Probability of Crashing", col = 'light green')
```
```{r}
probabilities <- predict(model2, rf_test, type = "response")
predicted.classes <- ifelse(probabilities > 0.5, 1, 0)
rf_test$pred.class <- predicted.classes
table("Predictions" = rf_test$pred.class, "Actual" = rf_test$TARGET_FLAG)
```
```{r}
confusionMatrix(as.factor(predicted.classes), as.factor(rf_test$TARGET_FLAG), positive = '1')

```


```{r}
curve <- roc(response = rf_test$TARGET_FLAG, 
    predictor = predicted.classes, 
    plot = TRUE, 
    print.auc = TRUE, 
    main = "ROC Curve")

```



## Select A Model
We will use the Stepwise regression models for both questions, as the logistic stepwise model was the most accurate, had the highest sensitivity, and was the least complex. The stepwise linear model also had a higher adjusted R^2 value, and a smaller p-value. Thus, these should be our best bet in imputing the target values in the evaluation dataset.

```{r}
eval <- read_csv('https://raw.githubusercontent.com/aaitelmouden/DATA621/master/Homework4/data/insurance-evaluation-data.csv')

# Get rid of INDEX, TARGET_FLAG, TARGET_AMT
eval <- subset(eval, select = -c(INDEX))

# Select money variables to clean before summary

eval2 <- eval[, names(eval) %in% c("INCOME", "OLDCLAIM", "HOME_VAL", "BLUEBOOK")]

# Clean using string function

eval2 <- apply(eval2, 2, function(y) gsub("\\$","",y))
eval2 <- apply(eval2, 2, function(y) gsub(",","",y))
eval2 <- apply(eval2, 2, as.integer)

# Combine variables

eval1 <- eval[, !names(eval) %in% c("INCOME", "OLDCLAIM", "HOME_VAL", "BLUEBOOK")]
eval1 <- cbind(eval1, eval2)

# Select characters variables to clean before summary

eval2 <- eval1[, names(eval1) %in% c("MSTATUS", "SEX", "EDUCATION", "JOB", "CAR_TYPE", "URBANICITY")]

# Clean using character function

eval2 <- apply(eval2, 2, function(y) gsub("z_","",y))

eval1 <- eval1[,!names(eval1) %in% c("MSTATUS", "SEX", "EDUCATION", "JOB", "CAR_TYPE", "URBANICITY")]
eval1 <- cbind(eval1, eval2)
head(eval1)
```


```{r}
probabilities <- predict(model2, eval1, type = "response")
preds <- ifelse(probabilities > 0.5, 1, 0)

eval1$TARGET_FLAG <- preds
eval1

```

```{r}
amt <- ifelse(eval1$TARGET_FLAG == 1, predict(lm_model2, eval1, type = "response"), 0)
eval1$TARGET_AMT <- amt

eval1
```
### Conclusion




## Appendix 












